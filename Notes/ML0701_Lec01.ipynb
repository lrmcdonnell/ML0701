{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Supervised Learning\n",
        "- training data is labeled\n",
        "- ex. spam detection\n",
        "\n",
        "###Active Learning\n",
        "- leverage unlabeled data\n",
        "- pick informative examples to be labeled in order to minimize human input\n",
        "\n",
        "###Reinforcement learning\n",
        "- accommodates indirect or delayed feedback\n",
        "\n"
      ],
      "metadata": {
        "id": "gZR-0DsN4GWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Classification: Decision Trees\n",
        "- **input**: labeled training examples {$x_i,y_i$}\n",
        "- unknown target function $f:x â†¦ y$\n",
        "- **output**: Hypothesis $h \\in H$ that best approximates target function $f$\n",
        "- each hypothesis $h$ is a decision tree\n",
        "\n",
        "How to find a good hypothesis for training data?\n",
        "- Algorithmic question\n",
        "\n",
        "When do we generalize and do well on unseen data?\n",
        "- Learning theory: quanitifies ability to generalize as a function of the amount of training data and the hypothesis space\n",
        "- Occam's razor: use the simplest hypothesis consistent with the data\n",
        "\n",
        "Fewer short hypotheses than long ones\n",
        "- short hypothesis that fits the data is less likely to be a statistical coincidence\n",
        "- highly probable that a sufficiently complex hypothesis will fit the data"
      ],
      "metadata": {
        "id": "6HjZqLSBLIjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Top-Down Induction of Decision Trees\n",
        "\n",
        "**ID3**: Natural greedy approach to growing decision tree top-down (from the root to the leaves by repeatedly replacing an existing leaf with an internal node)\n",
        "\n",
        "Algorithm:\n",
        "- Pick 'best' attribute to split at the root based on training data\n",
        "- Recurse on children that are impure (e.g., have both Yes and No)\n",
        "\n",
        "Which Attribute to select?\n",
        "- Information gain of A is the expected reduction in entropy if target variable Y for data sample S, due to sorting on variable A\n",
        "\n",
        "$Gain(S,A) = Entropy(S) - \\sum{\\frac{|S_v|}{|S|}Entropy(S_v)}$\n",
        "\n",
        "^ Original entropy subtracting with the expected entropy after S is partitioned using attribute A\n",
        "\n"
      ],
      "metadata": {
        "id": "FijY0MrXTnzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Entropy\n",
        "Entropy measures the impurity of S\n",
        "\n",
        "H(S) = $-p_+log_2p_+ - p_-log_2p_-$\n",
        "\n",
        "- proportion of positive examples, $p_+$\n",
        "\n",
        "- proportion of negative examples, $p_-$\n",
        "- ex., if all examples are negative or all positive, then entropy = 0\n",
        "- If 50/50 positive/negative, then entropy = 1\n",
        "\n",
        "If labels not Boolean,\n",
        "\n",
        "$H(S) = \\sum{-p_ilog_2p_i}$\n",
        "\n",
        "e.g., if all classes, all equally likely, then $H(S) = log_2c$\n",
        "\n",
        "$Entropy[9_+,5_-] = -\\frac{9}{14}log_2\\frac{9}{14} - \\frac{5}{14}log_2\\frac{5}{14} = 0.940$"
      ],
      "metadata": {
        "id": "CfFkTc1RWkp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Which is the best classifier?, Which attribute has the largest gain?\n",
        "- $Gain(S, Humidity) = 0.151$\n",
        "- $Gain(S, Wind) = 0.048$\n",
        "- $Gain(S, Outlook) = 0.246$\n",
        "- $Gain(S, Temperature) = 0.029$\n",
        "\n",
        "**Outlook** has the greatest information gain, and is thus the best classifier\n"
      ],
      "metadata": {
        "id": "Rk2ZyCwgYhci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Properties of ID3\n",
        "\n",
        "- ID3 performs heuristic search through space of decision trees\n",
        "- it stops at the smallest acceptable tree\n",
        "- can still overfit, but can be pruned by a validation set\n",
        "\n",
        "**Overfitting**\n",
        "\n",
        "overfits if $error_{true}(h) > error_{train}(h)$\n",
        "\n",
        "How to avoid overfitting:\n",
        "- stop growing when data split not statistically significant\n",
        "- grow full tree, then post-prune\n",
        "\n"
      ],
      "metadata": {
        "id": "CJKxvfDAZIXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Splitting Criterion\n",
        "- ID3: split the leaf that decreases the entropy the most\n",
        "\n",
        "Question: why not split according to the error rate?\n",
        "- Can get stuck in local minima"
      ],
      "metadata": {
        "id": "upqZceGXak_e"
      }
    }
  ]
}